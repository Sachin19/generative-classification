{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 19:30:36.655407: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-11 19:30:36.867465: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-11 19:30:38.410896: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/cudnn/cuda-9.1/7.1/cuda/lib64:/opt/cuda/11.1.1/extras/CUPTI/lib64/:/opt/cudnn/cuda-9.1/7.1/cuda/lib64:/opt/cuda/11.1.1/extras/CUPTI/lib64/::/opt/cuda/11.1.1:/opt/cuda/11.1.1\n",
      "2023-06-11 19:30:38.411167: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/cudnn/cuda-9.1/7.1/cuda/lib64:/opt/cuda/11.1.1/extras/CUPTI/lib64/:/opt/cudnn/cuda-9.1/7.1/cuda/lib64:/opt/cuda/11.1.1/extras/CUPTI/lib64/::/opt/cuda/11.1.1:/opt/cuda/11.1.1\n",
      "2023-06-11 19:30:38.411179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "special=False\n",
    "if special:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "else:\n",
    "    tokenizer.pad_token = tokenizer.eos_token    \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = {\"glue\": [\"cola\", \"sst2\", \"mnli\"], \"adv_glue\": [\"adv_sst2\"]}\n",
    "\n",
    "# datapath=\"tasksource/subjectivity\"\n",
    "# raw_dataset = load_dataset(datapath, cache_dir=\"datasets\")\n",
    "#\"jigsaw_unintended_bias\", data_dir=\"/projects/tir5/users/sachink/embed-style-transfer/data/toxicity-jigsaw/\", cache_dir=\"datasets\")\n",
    "\n",
    "# datapath=\"hyperpartisan\"\n",
    "raw_dataset = load_dataset(\"glue\", \"mnli\", cache_dir=\"datasets\", split=[\"validation\", ])\n",
    "\n",
    "print(raw_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['in his first stab at the form , jacquot takes a slightly anarchic approach that works only sporadically .',\n",
       "  'one long string of cliches .',\n",
       "  \"if you 've ever entertained the notion of doing what the title of this film implies , what sex with strangers actually shows may put you off the idea forever .\",\n",
       "  'k-19 exploits our substantial collective fear of nuclear holocaust to generate cheap hollywood tension .',\n",
       "  \"it 's played in the most straight-faced fashion , with little humor to lighten things up .\"],\n",
       " 'label': [2, 1, 1, 0, 1],\n",
       " 'label_text': ['neutral',\n",
       "  'negative',\n",
       "  'negative',\n",
       "  'very negative',\n",
       "  'negative']}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['validation'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008073568344116211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 8544,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f649bf756849748a3fc58d67913e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8544 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /projects/tir5/users/sachink/generative-classifiers/2023/datasets/SetFit___json/SetFit--sst5-4c07b9d5881ae209/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-66ed06f82f245d57.arrow\n",
      "Loading cached processed dataset at /projects/tir5/users/sachink/generative-classifiers/2023/datasets/SetFit___json/SetFit--sst5-4c07b9d5881ae209/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-2c9fef8d1b124dc1.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets and tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# label2id = {\"SUBJ\": 0, \"OBJ\":1}\n",
    "# def preprocess_function(examples):\n",
    "#         x = tokenizer(examples[\"Sentence\"], max_length=128, padding=True, truncation=True)   \n",
    "#         #print(examples['Label']) \n",
    "#         x['labels'] = [label2id[label] for label in examples['Label']]\n",
    "#         return x\n",
    "\n",
    "def preprocess_function(examples):\n",
    "        x = tokenizer(examples[\"text\"], max_length=128, truncation=True)   \n",
    "        return x\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\", \"label_text\"])\n",
    "# tokenized_dataset = tokenized_dataset.remove_columns([\"Sentence\", \"Solved conflict\", \"Label\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")\n",
    "print(\"datasets and tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(modelname)\n",
    "\n",
    "if special:\n",
    "    old_embeddings = model.get_input_embeddings()\n",
    "    num_old_embeddings, embedding_dim = old_embeddings.weight.size()\n",
    "    new_embeddings = torch.nn.Embedding(num_old_embeddings + 1, embedding_dim)\n",
    "\n",
    "    new_embeddings.to(old_embeddings.weight.device)\n",
    "\n",
    "    # initialize all new embeddings (in particular added tokens)\n",
    "    new_embeddings.weight.data.fill_(0.)\n",
    "\n",
    "    new_embeddings.weight.data[:num_old_embeddings, :] = old_embeddings.weight.data\n",
    "    model.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    model.vocab_size = num_old_embeddings + 1\n",
    "    model.config.vocab_size = model.vocab_size\n",
    "    \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1280, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_output_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This statement has a negative sentiment. ',\n",
       "  'This statement has a unhappy sentiment ',\n",
       "  'This statement has a neutral sentiment. ',\n",
       "  'This statement has a happy sentiment. ',\n",
       "  'This statement has an elelated sentiment. ']]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_alllabelstrings = [\n",
    "    [\"This statement has a negative sentiment. \"],\n",
    "    [\"This statement has a unhappy sentiment \"],\n",
    "    [\"This statement has a neutral sentiment. \"],\n",
    "    [\"This statement has a happy sentiment. \"],\n",
    "    [\"This statement has an elelated sentiment. \"]\n",
    "]\n",
    "\n",
    "# old_alllabelstrings = [\n",
    "#     [\"This is a one star review. \", \"This review receives a rating of one star. \"],\n",
    "#     [\"This is a two star review. \", \"This review receives a rating of two star. \"],\n",
    "#     [\"This is a three star review. \", \"This review receives a rating of three star. \"],\n",
    "#     [\"This is a four star review. \", \"This review receives a rating of four star. \"],\n",
    "#     [\"This is a five star review. \", \"This review receives a rating of five star. \"]\n",
    "# ]\n",
    "\n",
    "alllabelstrings = []\n",
    "for item in zip(*old_alllabelstrings):\n",
    "    alllabelstrings.append(list(item))\n",
    "\n",
    "alllabelstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old_alllabelstrings = [\n",
    "#     [  \n",
    "#         \"This statement is subject to interpretation. \",\n",
    "#         # \"The grammatical unacceptability and linguistic incorrectness of this sentence are apparent. \",\n",
    "#         # \"This sentence does not meet the standards of grammatical acceptability and linguistic correctness. \",\n",
    "#         # \"It can be stated that this sentence is grammatically unacceptable and linguistically incorrect. \",\n",
    "#         # \"The grammatical inaccuracy and linguistic impropriety of this sentence are evident. \",\n",
    "#         # \"This sentence demonstrates grammatical unacceptability and linguistic incorrectness. \",\n",
    "#         # \"The grammatical invalidity and linguistic inaccuracy of this sentence are apparent. \",\n",
    "#         # \"One can observe that this sentence is grammatically unacceptable and linguistically incorrect. \",\n",
    "#         # \"This sentence does not satisfy the criteria for both grammatical acceptability and linguistic correctness. \",\n",
    "#         # \"It is evident that this sentence is grammatically unacceptable and linguistically incorrect. \"\n",
    "#     ],\n",
    "#     [  \n",
    "#         \"This statement is objectively always true. \",\n",
    "#         # \"The grammatical acceptability and linguistic correctness of this sentence are evident. \",\n",
    "#         # \"This sentence meets the standards of grammatical acceptability and linguistic correctness. \",\n",
    "#         # \"It can be stated that this sentence is both grammatically acceptable and linguistically correct. \",\n",
    "#         # \"The grammatical accuracy and linguistic propriety of this sentence are undeniable. \",\n",
    "#         # \"This sentence demonstrates grammatical acceptability and linguistic correctness. \",\n",
    "#         # \"The grammatical validity and linguistic accuracy of this sentence are apparent. \",\n",
    "#         # \"One can observe that this sentence is both grammatically acceptable and linguistically correct. \",\n",
    "#         # \"This sentence satisfies the criteria for both grammatical acceptability and linguistic correctness. \",\n",
    "#         # \"It is evident that this sentence is both grammatically acceptable and linguistically correct. \"\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# alllabelstrings = []\n",
    "# for item in zip(*old_alllabelstrings):\n",
    "#     alllabelstrings.append(list(item))\n",
    "\n",
    "# alllabelstrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[50256,  1212,  2643,   468,   257,  4633, 15598,    13,   220],\n",
       "         [50256, 50256,  1212,  2643,   468,   257, 19283, 15598,   220],\n",
       "         [50256,  1212,  2643,   468,   257,  8500, 15598,    13,   220],\n",
       "         [50256,  1212,  2643,   468,   257,  3772, 15598,    13,   220],\n",
       "         [ 1212,  2643,   468,   281,  9766, 17249, 15598,    13,   220]],\n",
       "        device='cuda:0'), 'attention_mask': tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token    \n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "num_labels = len(alllabelstrings[0])\n",
    "num_labelstrings = len(alllabelstrings)\n",
    "\n",
    "tokenizer.padding_side = \"left\"\n",
    "alllabelstrings_tokenized = []\n",
    "for labelstrings in alllabelstrings:\n",
    "    alllabelstrings_tokenized.append(tokenizer(labelstrings, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\").to(device))\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "alllabelstrings_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch, alllabelstrings_tokenized, i):\n",
    "    merged_labelstrings = alllabelstrings_tokenized[i]\n",
    "    # print(merged_labelstrings)\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    batch_size, seq_len = batch['input_ids'].size()\n",
    "    label_len = merged_labelstrings['input_ids'].size(-1)\n",
    "    # print(batch_size, seq_len, label_len)\n",
    "    \n",
    "    expanded_batch_input_ids = batch['input_ids'].repeat_interleave(merged_labelstrings['input_ids'].size(0), dim=0) # output size = (#labels*batch_size, L)\n",
    "    expanded_label_input_ids = merged_labelstrings['input_ids'].view(1, -1, label_len).expand(batch_size, -1, -1).contiguous().view(-1, label_len)\n",
    "    input_ids = torch.cat([expanded_label_input_ids, expanded_batch_input_ids], dim=1)\n",
    "\n",
    "\n",
    "    expanded_batch_attention_mask = batch['attention_mask'].repeat_interleave(merged_labelstrings['attention_mask'].size(0), dim=0) # output size = (#labels*batch_size, L)\n",
    "    expanded_label_attention_mask = merged_labelstrings['attention_mask'].view(1, -1, label_len).expand(batch_size, -1, -1).contiguous().view(-1, label_len)\n",
    "    attention_mask = torch.cat([expanded_label_attention_mask, expanded_batch_attention_mask], dim=1)\n",
    "     \n",
    "    labels = batch['labels']\n",
    "    batch['input_ids'] = input_ids\n",
    "    batch['attention_mask'] = attention_mask\n",
    "    bsz = input_ids.size(0)\n",
    "    if \"labels\" in batch:\n",
    "        del batch['labels']\n",
    "\n",
    "    # if \"Label\" in batch:\n",
    "        # del batch['Label']\n",
    "    # if \"Solved conflict\" in batch:\n",
    "    #     del batch['Solved conflict']\n",
    "    # del batch['Label']\n",
    "    # del batch['Solved conflict']\n",
    "\n",
    "    return batch, labels, batch_size, label_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_collator.tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "eval_dataloader = DataLoader(tokenized_dataset['validation'], collate_fn=data_collator, batch_size=4)\n",
    "\n",
    "# print(tokenized_dataset['test'])\n",
    "batch = next(iter(eval_dataloader))\n",
    "\n",
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 276/276 [00:18<00:00, 14.67it/s]\n"
     ]
    }
   ],
   "source": [
    "label2id = {}\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "accurate = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "############################################\n",
    "for batch in tqdm(eval_dataloader):\n",
    "# if True:\n",
    "    #print(batch)\n",
    "    nlls = []\n",
    "    for i in range(len(alllabelstrings_tokenized)):\n",
    "        new_batch, labels, batch_size, label_len = process_batch(batch, alllabelstrings_tokenized, i)\n",
    "        # for i in range(new_batch['input_ids'].size(0)):\n",
    "        #     print(tokenizer.decode(new_batch['input_ids'][i]))\n",
    "        # print(new_batch)\n",
    "        #print(new_batch['input_ids'].size())\n",
    "        outputs = model(**new_batch)\n",
    "        logits1 = outputs.logits\n",
    "        \n",
    "        shift_logprobs = torch.nn.functional.log_softmax(logits1[..., label_len-1:-1, :], dim=-1).contiguous()\n",
    "        shift_target = new_batch['input_ids'][..., label_len:].contiguous()\n",
    "\n",
    "        # prefix_logprobs =\n",
    "        # prefix_target = \n",
    "\n",
    "        nll = torch.nn.functional.nll_loss(shift_logprobs.view(-1, shift_logprobs.size(-1)), shift_target.view(-1), reduction=\"none\", ignore_index=tokenizer.pad_token_id).view(-1, shift_target.size(-1))\n",
    "        nll = nll.sum(dim=-1)/shift_target.ne(tokenizer.pad_token_id).float().sum(dim=-1)\n",
    "\n",
    "        # print(nll.view(batch_size, -1))\n",
    "        # nll = -torch.logsumexp(-nll.view(batch_size, num_labels, num_labelstrings[0]), dim=-1) + np.log(num_labelstrings[0])\n",
    "        # nll = nll.view(batch_size, num_labels, num_labelstrings[0])#.mean(dim=-1)\n",
    "        nll = nll.view(batch_size, num_labels)\n",
    "        # print(nll)\n",
    "        # print(nll.min(dim=1)[1].eq(batch['labels'].cuda()).int().sum().item())\n",
    "        # print(nll[1])\n",
    "\n",
    "        nlls.append(nll)\n",
    "    \n",
    "    nll = torch.stack(nlls, dim=2)\n",
    "    nll = -torch.logsumexp(-nll, dim=-1) + np.log(num_labelstrings)\n",
    "\n",
    "    #print(nll)\n",
    "    nll = nll.min(dim=1)\n",
    "    # print(nll[1])\n",
    "\n",
    "    accurate += nll[1].eq(batch['labels'].cuda()).int().sum().item()\n",
    "    total += nll[1].size(0)\n",
    "    all_predictions += nll[1].tolist()\n",
    "    all_labels += batch['labels'].tolist()\n",
    "    #print(all_predictions)\n",
    "    #print(all_labels)\n",
    "    # print(accurate, \"/\", nll[1].size(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/1101, 0.14986376021798364\n",
      "[[  2   0   1   0 136]\n",
      " [  3   0   6   0 280]\n",
      " [  1   0   3   2 223]\n",
      " [  2   0   2   1 274]\n",
      " [  1   0   3   2 159]]\n",
      "2244\n"
     ]
    }
   ],
   "source": [
    "print(f\"{accurate}/{total}, {accurate/total}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "print(sum(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alllabelstrings = [\n",
    "# #     [  \n",
    "# #         \"This review exhibits a negative bias.\",\n",
    "# #         \"The inclination of this review is towards the negative side.\",\n",
    "# #         \"There is a unfavorable slant in this review.\",\n",
    "# #         \"The overall tone of this review is negative.\",\n",
    "# #         \"This review shows a leaning against the subject.\",\n",
    "# #         \"There is a negative inclination in this review.\",\n",
    "# #         \"The overall impression of this review is pessimistic.\",\n",
    "# #         \"This review tends to disfavor the subject being discussed.\",\n",
    "# #         \"There is a negative inclination evident in this review.\",\n",
    "# #         \"The general sentiment of this review is negative.\",\n",
    "# #     ],\n",
    "# #     [\n",
    "# #         \"This review exhibits a positive bias.\",\n",
    "# #         \"The inclination of this review is towards the positive side.\",\n",
    "# #         \"There is a favorable slant in this review.\",\n",
    "# #         \"The overall tone of this review is positive.\",\n",
    "# #         \"This review shows a leaning in favor of the subject.\",\n",
    "# #         \"There is a positive inclination in this review.\",\n",
    "# #         \"The overall impression of this review is optimistic.\",\n",
    "# #         \"This review tends to favor the subject being discussed.\",\n",
    "# #         \"There is a positive inclination evident in this review.\",\n",
    "# #         \"The general sentiment of this review is positive.\",\n",
    "# #     ]\n",
    "# # ]\n",
    "\n",
    "# # alllabelstrings = [[\"This review is leaning negative. \"], [\"This review is leaning positive. \"]]\n",
    "# # alllabelstrings = [[\"This review criticizes. \"], [\"This review appreciates. \"]]\n",
    "\n",
    "# alllabelstrings = [\n",
    "#     [  \n",
    "#         \"This review exhibits a negative intent.\",\n",
    "#         #\"The inclination of this review is towards the negative side.\",\n",
    "#         #\"There is a unfavorable slant in this review.\",\n",
    "#         # \"The overall tone of this review is negative.\",\n",
    "#         # \"This review shows a leaning against the subject.\",\n",
    "#         \"There is a negative inclination in this review.\",\n",
    "#         # \"The overall impression of this review is pessimistic.\",\n",
    "#         # \"This review tends to disfavor the subject being discussed.\",\n",
    "#         # \"There is a negative inclination evident in this review.\",\n",
    "#         # \"The general sentiment of this review is negative.\",\n",
    "#     ],\n",
    "#     [\n",
    "#         \"This review exhibits a positive intent.\",\n",
    "#         #\"The inclination of this review is towards the positive side.\",\n",
    "#         #\"There is a favorable slant in this review.\",\n",
    "#         # \"The overall tone of this review is positive.\",\n",
    "#         # \"This review shows a leaning in favor of the subject.\",\n",
    "#         \"There is a positive inclination in this review.\",\n",
    "#         # \"The overall impression of this review is optimistic.\",\n",
    "#         # \"This review tends to favor the subject being discussed.\",\n",
    "#         # \"There is a positive inclination evident in this review.\",\n",
    "#         # \"The general sentiment of this review is positive.\",\n",
    "#     ]\n",
    "# ]\n",
    "\n",
    "# num_labels = len(alllabelstrings)\n",
    "# num_labelstrings = [len(labelstrings) for labelstrings in alllabelstrings]\n",
    "\n",
    "# merged_labelstrings = []\n",
    "# for labelstrings in alllabelstrings:\n",
    "#     merged_labelstrings += labelstrings\n",
    "\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# merged_labelstrings_tokenized = tokenizer(merged_labelstrings, add_special_tokens=False, padding=True, return_tensors=\"pt\").to(device)\n",
    "# tokenizer.padding_side = \"right\"\n",
    "\n",
    "# print(len(merged_labelstrings))\n",
    "# print(merged_labelstrings_tokenized['input_ids'].size())\n",
    "# for key in merged_labelstrings_tokenized:\n",
    "#     merged_labelstrings_tokenized[key] = merged_labelstrings_tokenized[key].view(num_labels, num_labelstrings[0], -1)\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token    \n",
    "# tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
